{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/konderal333/HGT-2022-EmDomArDon/blob/main/bert2bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load models and data"
      ],
      "metadata": {
        "id": "l935bl02XlAr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wf0Ee_SOf0qi",
        "outputId": "e8565ba7-a982-4b75-918a-c3ed5e0d4a66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'HGT-2022-EmDomArDon' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/konderal333/HGT-2022-EmDomArDon.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re"
      ],
      "metadata": {
        "id": "oLsjhiWggpKu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/HGT-2022-EmDomArDon/cikkek_10k_cleanedv1.csv')"
      ],
      "metadata": {
        "id": "gcCdIAFigtO1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.isnull().values.any())\n",
        "print(sum(df.duplicated()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_jetG8guxLJ",
        "outputId": "10ed9e5b-07df-4818-b652-59f21c9bf321"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna()\n",
        "df = df.drop_duplicates()"
      ],
      "metadata": {
        "id": "F9Wnh0wXvTnK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.apply(lambda x: x.str.strip())\n",
        "df['Textbody'] = df['Textbody'].apply(lambda x: re.sub(r\"(\\.)([A-Z])\", r'\\1 \\2', str(x))) # add missing whitespace between sentences\n",
        "\n",
        "df['Lead'] = df['Title'] + '. ' + df['Headline'] # maybe only headline and drop title?\n",
        "df = df.drop(['Title', 'Headline'],axis=1)\n",
        "df.head()\n",
        "\n",
        "#reindexing the dataframe\n",
        "df.reset_index(inplace=True)\n",
        "df = df.drop(['index'], axis=1)"
      ],
      "metadata": {
        "id": "a_Wq2IDmk9oL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XZHCm-dgwKA",
        "outputId": "d3a612d8-b722-44df-eee1-f09a7a8b0bb5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.24.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('SZTAKI-HLT/hubert-base-cc')"
      ],
      "metadata": {
        "id": "rO9me5lwgx9z"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bert2bert\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1jVivzwtgoZx38yMeZA5BthGGvoPxBZk5' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1jVivzwtgoZx38yMeZA5BthGGvoPxBZk5\" -O model_bert2bert.tar.gz && rm -rf /tmp/cookies.txt\n",
        "\n",
        "# model\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1GEdOmmKZEJOD2Ei28FtVEf534-nBEcBd' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1GEdOmmKZEJOD2Ei28FtVEf534-nBEcBd\" -O model.tar.gz && rm -rf /tmp/cookies.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "520a87d3-a16a-4e30-a787-a13079937e04",
        "id": "OJp93I3EteNc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-26 10:55:32--  https://docs.google.com/uc?export=download&confirm=t&id=1jVivzwtgoZx38yMeZA5BthGGvoPxBZk5\n",
            "Resolving docs.google.com (docs.google.com)... 172.253.117.139, 172.253.117.102, 172.253.117.113, ...\n",
            "Connecting to docs.google.com (docs.google.com)|172.253.117.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-0o-18-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/a3sdd38gc3a2dr0bcmr70qrjlodsdvtd/1669460100000/03710960119062529382/*/1jVivzwtgoZx38yMeZA5BthGGvoPxBZk5?e=download&uuid=587b8929-03eb-4b81-97d0-6a48fbca35a0 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-11-26 10:55:32--  https://doc-0o-18-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/a3sdd38gc3a2dr0bcmr70qrjlodsdvtd/1669460100000/03710960119062529382/*/1jVivzwtgoZx38yMeZA5BthGGvoPxBZk5?e=download&uuid=587b8929-03eb-4b81-97d0-6a48fbca35a0\n",
            "Resolving doc-0o-18-docs.googleusercontent.com (doc-0o-18-docs.googleusercontent.com)... 74.125.135.132, 2607:f8b0:400e:c01::84\n",
            "Connecting to doc-0o-18-docs.googleusercontent.com (doc-0o-18-docs.googleusercontent.com)|74.125.135.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2648670050 (2.5G) [application/x-gzip]\n",
            "Saving to: ‘model_bert2bert.tar.gz’\n",
            "\n",
            "model_bert2bert.tar 100%[===================>]   2.47G   167MB/s    in 12s     \n",
            "\n",
            "2022-11-26 10:55:45 (206 MB/s) - ‘model_bert2bert.tar.gz’ saved [2648670050/2648670050]\n",
            "\n",
            "--2022-11-26 10:55:45--  https://docs.google.com/uc?export=download&confirm=t&id=1GEdOmmKZEJOD2Ei28FtVEf534-nBEcBd\n",
            "Resolving docs.google.com (docs.google.com)... 172.253.117.139, 172.253.117.102, 172.253.117.113, ...\n",
            "Connecting to docs.google.com (docs.google.com)|172.253.117.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-10-18-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/curmu5mjv4vepjjaibsuos3s4fs1lgb8/1669460100000/03710960119062529382/*/1GEdOmmKZEJOD2Ei28FtVEf534-nBEcBd?e=download&uuid=eb8d9b2d-d332-4ec3-99c8-44d00228ca2c [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-11-26 10:55:45--  https://doc-10-18-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/curmu5mjv4vepjjaibsuos3s4fs1lgb8/1669460100000/03710960119062529382/*/1GEdOmmKZEJOD2Ei28FtVEf534-nBEcBd?e=download&uuid=eb8d9b2d-d332-4ec3-99c8-44d00228ca2c\n",
            "Resolving doc-10-18-docs.googleusercontent.com (doc-10-18-docs.googleusercontent.com)... 74.125.135.132, 2607:f8b0:400e:c01::84\n",
            "Connecting to doc-10-18-docs.googleusercontent.com (doc-10-18-docs.googleusercontent.com)|74.125.135.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 927224269 (884M) [application/x-gzip]\n",
            "Saving to: ‘model.tar.gz’\n",
            "\n",
            "model.tar.gz        100%[===================>] 884.27M   213MB/s    in 4.4s    \n",
            "\n",
            "2022-11-26 10:55:50 (199 MB/s) - ‘model.tar.gz’ saved [927224269/927224269]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.unpack_archive('model.tar.gz', 'model')\n",
        "shutil.unpack_archive('model_bert2bert.tar.gz', 'bert2bert')"
      ],
      "metadata": {
        "id": "J5A7_9vkteNf"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import EncoderDecoderModel\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "bert2bert_model_path = '/content/bert2bert/model/pytorch_model.bin'\n",
        "bert2bert_config = '/content/bert2bert/model/config.json'\n",
        "\n",
        "model_path = '/content/model/model/pytorch_model.bin'\n",
        "config = '/content/model/model/config.json'\n",
        "\n",
        "#bert2bert = EncoderDecoderModel.from_pretrained(bert2bert_model_path, config=bert2bert_config).to(device)\n",
        "model = EncoderDecoderModel.from_pretrained(model_path, config=config).to(device)"
      ],
      "metadata": {
        "id": "1gftlAZQ-DKl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://colab.research.google.com/drive/1WIk2bxglElfZewOHboPFNj8H44_VAyKE?usp=sharing&fbclid=IwAR2-pdnudSutvgqIlUUg4NP1Q0wJO9NQbteg_wVL9n8gsS2yMTfMp9YfC6w#scrollTo=bYmdx-W1NAky"
      ],
      "metadata": {
        "id": "VvDFzCOSq8hH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data stats"
      ],
      "metadata": {
        "id": "BhouBZMdXovK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # map article and summary len to dict as well as if sample is longer than 512 tokens\n",
        "# def map_to_length(x):\n",
        "#   x[\"article_len\"] = len(tokenizer(x[\"Textbody\"]).input_ids)\n",
        "#   x[\"article_longer_512\"] = int(x[\"article_len\"] > 512)\n",
        "#   x[\"summary_len\"] = len(tokenizer(x[\"Lead\"]).input_ids)\n",
        "#   x[\"summary_longer_64\"] = int(x[\"summary_len\"] > 64)\n",
        "#   x[\"summary_longer_128\"] = int(x[\"summary_len\"] > 128)\n",
        "#   return x"
      ],
      "metadata": {
        "id": "57EXGHFigCvi"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data_stats = df.apply(map_to_length, axis=1)"
      ],
      "metadata": {
        "id": "M0vCx9Jnj23d"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def compute_and_print_stats(x):\n",
        "#   print(\n",
        "#       \"Article Mean: {}, %-Articles > 512:{}, Summary Mean:{}, %-Summary > 64:{}, %-Summary > 128:{}\".format(\n",
        "#           x[\"article_len\"],\n",
        "#           x[\"article_longer_512\"],\n",
        "#           x[\"summary_len\"],\n",
        "#           x[\"summary_longer_64\"],\n",
        "#           x[\"summary_longer_128\"],\n",
        "#       )\n",
        "#   )"
      ],
      "metadata": {
        "id": "EjYlkZACj26P"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# output = data_stats.apply(compute_and_print_stats, axis=1)"
      ],
      "metadata": {
        "id": "evpWLoX8mQMy"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Function to handle larger inputs than 512 tokens"
      ],
      "metadata": {
        "id": "yl-We1JlC_bD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install quntoken\n",
        "from quntoken import tokenize   #module to break text into sentences\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "gmJpNVgTDREn",
        "outputId": "66b11377-282a-4fc0-e745-7fb48f22a682",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting quntoken\n",
            "  Downloading quntoken-3.1.8-py3-none-any.whl (9.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.9 MB 7.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: quntoken\n",
            "Successfully installed quntoken-3.1.8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dict_of_tokens = {} \n",
        "#this dict will contain the article title as keys and the tokenized lists from the function below as values\n",
        "\n",
        "def text_to_tokens(art_title, x):      \n",
        "  #insert the text to tokenize; first breaks text into sentences, then makes a list of lists with the i-th element being the tokenization of the i-th sentence\n",
        "  #dictionary is being used for faster computation and easier accessibility\n",
        "  sentence_list = []\n",
        "  token_list = []\n",
        "\n",
        "  x = re.sub(r\"(\\.)([A-Z])\", r'\\1 \\2', x) # fixes missings whitespaces between sentences\n",
        " \n",
        "  for sen in nltk.tokenize.sent_tokenize(x):\n",
        "    sentence_list.append(sen)\n",
        "    token_list.append(tokenizer.tokenize(sen))\n",
        "\n",
        "  dict_of_tokens[art_title] = token_list\n",
        "\n",
        "# making the BERT tokenized sentences from each article\n",
        "for i in range(len(df.Lead)):\n",
        "  text_to_tokens(df.Lead[i].strip(), df.Textbody[i])"
      ],
      "metadata": {
        "id": "Q0tqz2DfDHyT"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some basic statistics about the number of tokens"
      ],
      "metadata": {
        "id": "Z_jZfSbsDWjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import statistics\n",
        "\n",
        "number_of_tokens_all = []\n",
        "\n",
        "for tokens in dict_of_tokens.values():\n",
        "  length = 0\n",
        "  for lists in tokens:\n",
        "    length += len(lists)\n",
        "  number_of_tokens_all.append(length)\n"
      ],
      "metadata": {
        "id": "yldWq4hgDVkI"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(number_of_tokens_all, bins=100, density=False)\n",
        "plt.title('Distribution of number of tokens generated from articles')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Fj5dC77JDZXZ",
        "outputId": "f3e131c7-b690-47da-d7b6-faf898b2568b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Median of tokens: \", statistics.median(number_of_tokens_all))\n",
        "print(\"Mean of tokens: \", statistics.mean(number_of_tokens_all))"
      ],
      "metadata": {
        "id": "qp1FCUJCDaES",
        "outputId": "66e31a3d-9707-4388-9d42-0278a1284197",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Median of tokens:  420.0\n",
            "Mean of tokens:  765.9577029618581\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# number of articles with tokens more that 510:\n",
        "print(np.sum(np.asarray(number_of_tokens_all) > 510))"
      ],
      "metadata": {
        "id": "QbLh648cDcFw",
        "outputId": "81dd07a7-f501-4bc9-b722-af7e1bf6b5b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3749\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process data"
      ],
      "metadata": {
        "id": "NvtkSH6FXrZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_data_to_model_inputs(batch):\n",
        "        # Tokenize the input and target data\n",
        "        inputs = tokenizer(batch['Textbody'], padding='max_length', truncation=True, max_length=512)\n",
        "        outputs = tokenizer(batch['Lead'], padding='max_length', truncation=True, max_length=512)\n",
        "\n",
        "        batch['input_ids'] = inputs.input_ids\n",
        "        batch['attention_mask'] = inputs.attention_mask\n",
        "        batch['decoder_input_ids'] = outputs.input_ids\n",
        "        batch['decoder_attention_mask'] = outputs.attention_mask\n",
        "        batch['labels'] = outputs.input_ids.copy()\n",
        "\n",
        "        # batch['labels'] = [[-100 if token == tokenizer.pad_token_id else token for token in labels]\n",
        "        #                    for labels in batch['labels']]\n",
        "        batch['labels'] = [-100 if token == tokenizer.pad_token_id else token for token in batch['labels']]\n",
        "\n",
        "        return batch"
      ],
      "metadata": {
        "id": "uBAQRVkUeV3d"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# just to make it faster\n",
        "train_data = df.iloc[0:20]\n",
        "val_data = df.iloc[20:30]\n",
        "test_data = df.iloc[30:40]\n",
        "\n",
        "# resetting the indeces\n",
        "train_data.reset_index(inplace=True)\n",
        "val_data.reset_index(inplace=True)\n",
        "test_data.reset_index(inplace=True)\n",
        "\n",
        "train_data = train_data.drop(['index'], axis=1)\n",
        "val_data = val_data.drop(['index'], axis=1)\n",
        "test_data = test_data.drop(['index'], axis=1)"
      ],
      "metadata": {
        "id": "0koyJhZXmQPF"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = train_data.apply(process_data_to_model_inputs, axis=1)\n",
        "train_data = train_data.drop(['Textbody', 'Lead'], axis=1)\n",
        "\n",
        "val_data = val_data.apply(process_data_to_model_inputs, axis=1)\n",
        "val_data = val_data.drop(['Textbody', 'Lead'], axis=1)"
      ],
      "metadata": {
        "id": "vPUFBFq3jXip"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "Voqr9PbmvebJ",
        "outputId": "39c94749-7834-4529-bd65-6a6f79a59a57"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                           input_ids  \\\n",
              "0  [2, 2155, 7155, 2524, 4046, 4341, 15269, 14826...   \n",
              "1  [2, 30196, 9688, 11787, 11628, 2005, 11149, 22...   \n",
              "2  [2, 4405, 2546, 8984, 3576, 2079, 2005, 16667,...   \n",
              "3  [2, 22158, 6688, 29536, 2005, 13549, 31735, 12...   \n",
              "4  [2, 10708, 11447, 2685, 2066, 9763, 31736, 200...   \n",
              "\n",
              "                                      attention_mask  \\\n",
              "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
              "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
              "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
              "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
              "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
              "\n",
              "                                   decoder_input_ids  \\\n",
              "0  [2, 21086, 2018, 6480, 2045, 26204, 14350, 804...   \n",
              "1  [2, 28589, 6997, 2039, 5833, 6505, 2094, 2005,...   \n",
              "2  [2, 5388, 28306, 7996, 2005, 16667, 3021, 2653...   \n",
              "3  [2, 12930, 12887, 6704, 7671, 2837, 20664, 297...   \n",
              "4  [2, 2930, 29814, 2066, 9763, 2033, 2718, 31741...   \n",
              "\n",
              "                              decoder_attention_mask  \\\n",
              "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
              "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
              "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
              "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
              "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
              "\n",
              "                                              labels  \n",
              "0  [2, 21086, 2018, 6480, 2045, 26204, 14350, 804...  \n",
              "1  [2, 28589, 6997, 2039, 5833, 6505, 2094, 2005,...  \n",
              "2  [2, 5388, 28306, 7996, 2005, 16667, 3021, 2653...  \n",
              "3  [2, 12930, 12887, 6704, 7671, 2837, 20664, 297...  \n",
              "4  [2, 2930, 29814, 2066, 9763, 2033, 2718, 31741...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-da749d1f-8a0b-4a9f-80bb-e756d2738d92\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>input_ids</th>\n",
              "      <th>attention_mask</th>\n",
              "      <th>decoder_input_ids</th>\n",
              "      <th>decoder_attention_mask</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[2, 2155, 7155, 2524, 4046, 4341, 15269, 14826...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[2, 21086, 2018, 6480, 2045, 26204, 14350, 804...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[2, 21086, 2018, 6480, 2045, 26204, 14350, 804...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[2, 30196, 9688, 11787, 11628, 2005, 11149, 22...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[2, 28589, 6997, 2039, 5833, 6505, 2094, 2005,...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[2, 28589, 6997, 2039, 5833, 6505, 2094, 2005,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[2, 4405, 2546, 8984, 3576, 2079, 2005, 16667,...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[2, 5388, 28306, 7996, 2005, 16667, 3021, 2653...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[2, 5388, 28306, 7996, 2005, 16667, 3021, 2653...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[2, 22158, 6688, 29536, 2005, 13549, 31735, 12...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[2, 12930, 12887, 6704, 7671, 2837, 20664, 297...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[2, 12930, 12887, 6704, 7671, 2837, 20664, 297...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[2, 10708, 11447, 2685, 2066, 9763, 31736, 200...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[2, 2930, 29814, 2066, 9763, 2033, 2718, 31741...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[2, 2930, 29814, 2066, 9763, 2033, 2718, 31741...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-da749d1f-8a0b-4a9f-80bb-e756d2738d92')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-da749d1f-8a0b-4a9f-80bb-e756d2738d92 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-da749d1f-8a0b-4a9f-80bb-e756d2738d92');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "i2Ntmb6Bi9bY"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# converting to arrow dataset (easier to use)\n",
        "import pyarrow as pa\n",
        "import pyarrow.dataset as ds\n",
        "from datasets import Dataset\n",
        "\n",
        "train_data = Dataset(pa.Table.from_pandas(train_data))\n",
        "val_data = Dataset(pa.Table.from_pandas(val_data))\n",
        "\n",
        "# format type\n",
        "train_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yz9aXIWvguOJ",
        "outputId": "e18c3afe-902f-45d3-e4b4-b76505ad4e03"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'attention_mask', 'decoder_input_ids', 'decoder_attention_mask', 'labels'],\n",
              "    num_rows: 20\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#converting to tensor\n",
        "train_data.set_format(\n",
        "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
        ")\n",
        "\n",
        "val_data.set_format(\n",
        "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
        ")"
      ],
      "metadata": {
        "id": "AtoQw3txhJye"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#old function\n",
        "\n",
        "# def to_tensor(x):\n",
        "#   for col in x.keys():\n",
        "#     x[col] = torch.Tensor(x[col])\n",
        "#   return x\n",
        "\n",
        "# train_data = train_data.apply(to_tensor)\n",
        "# val_data = val_data.apply(to_tensor)"
      ],
      "metadata": {
        "id": "jBwrVr5xvvVF"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example\n",
        "train_data[\"labels\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xz2hUNUaf7Mq",
        "outputId": "760ed8ef-dd6c-4fe2-de93-1dea1bab9c1c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[    2, 21086,  2018,  ...,  -100,  -100,  -100],\n",
              "        [    2, 28589,  6997,  ...,  -100,  -100,  -100],\n",
              "        [    2,  5388, 28306,  ...,  -100,  -100,  -100],\n",
              "        ...,\n",
              "        [    2,  2038, 25816,  ...,  -100,  -100,  -100],\n",
              "        [    2, 13639,  3412,  ...,  -100,  -100,  -100],\n",
              "        [    2,  9897, 15234,  ...,  -100,  -100,  -100]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict"
      ],
      "metadata": {
        "id": "nE1kDLgVYQn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_summary(batch):\n",
        "    # cut off at BERT max length 512\n",
        "    inputs = tokenizer(batch['Textbody'], padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n",
        "    input_ids = inputs.input_ids.to('cuda')\n",
        "    attention_mask = inputs.attention_mask.to('cuda')\n",
        "\n",
        "    outputs = model.generate(input_ids, attention_mask=attention_mask) # the same doesn't work with bert2bert, why?? Donát: it works, but isn't very good\n",
        "\n",
        "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "    batch['pred_summary'] = output_str\n",
        "\n",
        "    return batch"
      ],
      "metadata": {
        "id": "9-bSOnCrM1Ct"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example\n",
        "batch = df.iloc[10]\n",
        "batch['Textbody']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "dkfyOxW0Q-rG",
        "outputId": "e2ab3107-0ee5-40f7-c7fb-a9cd48fed060"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Találkozni szeretett volna a Kölcsey Ferenc Gimnázium tanárait menesztő tankerületi vezetővel Soproni Tamás, terézvárosi polgármester. Marosi Betrix azonban nem látta indokoltnak, hogy beszélgessenek. A tankerületi vezető levelét Facebook-oldalán tette közzé a Momentum politikusa. Azt írja Marosi, hogy „természetesen a tankerületi központnak is az az érdeke, hogy az iskolában a helyzet normalizálódjon, a pedagógusok tanítsanak, a diákok tanuljanak”. A nevelés-oktatás feltételeinek biztosítása a VI. kerület állami fenntartású iskoláiban a tankerület feladata és felelőssége,„a tankerületi központ sem kíván beleszólni abba, hogy az Önkormányzat hogyan és kikkel látja el a feladatait.”A tankerületi vezető nem látja indokoltnak, hogy személyesen is találkozzon a kerület polgármesterével, mert a munkáltatói intézkedés „kizárólag a hatályos jogszabályokból fakadó lépés volt”. Marosi szerint az érintett pedagógusok a többszöri figyelmeztetés ellenére is visszatérően megszegték „a pedagógus jogviszonyból fakadó kötelezettségeket”.„Természetesen köszönettel veszem Tisztelt Polgármester Úr közreműködését, amennyiben az arra irányul, hogy a pedagógusok a jogszabályokban számukra biztosított, törvényes módon fejezzék ki véleményüket, úgy, hogy az a gyermekek tanuláshoz való jogának érvényesülését ne korlátozza” – fogalmaz a levélben. Soproni azért kezdeményezett beszélgetést, mert aggasztja a kerületi diákok és tanárok sorsa. „Mi tesszük tovább a dolgunkat: százmilliókkal támogatjuk a helyi iskolák felújítását, cserébe csak a tanárok és diákok felé mutatott empatikus hozzáállást várnánk” – írja a Facebook-bejegyzésében. Múlt pénteken a VI. kerületi Kölcsey Ferenc Gimnázium több, a polgári engedetlenségben résztvevő tanárát azonnali hatállyal menesztették. A tankerület azt írta a Telexnek, hogy az érintett tanárok felmentése indokolt, mert a tanárok írásbeli figyelmeztetés után is többször megismételték „jogszerűtlen cselekedetüket”. Még aznap több százan tiltakoztak a döntés ellen a gimnázium előtt, majd a héten tovább folytatódtak a tüntetések. A menesztett tanárok munkaügyi pert indítanak.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now the summary\n",
        "inputs = tokenizer(batch['Textbody'], padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n",
        "input_ids = inputs.input_ids.to('cuda')\n",
        "attention_mask = inputs.attention_mask.to('cuda')\n",
        "outputs = model.generate(input_ids, attention_mask=attention_mask, max_length=50)\n",
        "output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "output_str"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrqvfI-TRIXK",
        "outputId": "c0efab82-dd94-4dc8-d180-b0e2aac5bdce"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Soproni Tamás szerint a tankerületi központnak is az az érdeke, hogy a tanárok és diákok tanuljanak.']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's see the tokens\n",
        "for t in input_ids:\n",
        "    print(tokenizer.convert_ids_to_tokens(t))"
      ],
      "metadata": {
        "id": "7MuKwvBcBe5I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dd68d20-728a-4185-b4d8-79fc7082da96"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', 'Találkoz', '##ni', 'szeretett', 'volna', 'a', 'Kölcs', '##ey', 'Ferenc', 'Gimnázium', 'tanára', '##it', 'men', '##esztő', 'tank', '##erületi', 'vezető', '##vel', 'Sopron', '##i', 'Tamás', ',', 'ter', '##éz', '##városi', 'polgármester', '.', 'Maros', '##i', 'Bet', '##rix', 'azonban', 'nem', 'látta', 'indokolt', '##nak', ',', 'hogy', 'beszélg', '##essenek', '.', 'A', 'tank', '##erületi', 'vezető', 'level', '##ét', 'Facebook', '-', 'oldalán', 'tette', 'közzé', 'a', 'Mo', '##ment', '##um', 'politikus', '##a', '.', 'Azt', 'írja', 'Maros', '##i', ',', 'hogy', '„', 'természetesen', 'a', 'tank', '##erületi', 'központ', '##nak', 'is', 'az', 'az', 'érdeke', ',', 'hogy', 'az', 'iskolában', 'a', 'helyzet', 'norm', '##alizál', '##ódjon', ',', 'a', 'pedagógusok', 'tanít', '##sanak', ',', 'a', 'diákok', 'tanul', '##janak', '”', '.', 'A', 'nevelés', '-', 'oktatás', 'feltételeinek', 'biztosítása', 'a', 'VI', '.', 'kerület', 'állami', 'fenntart', '##ású', 'iskol', '##áiban', 'a', 'tank', '##erület', 'feladata', 'és', 'felelőssége', ',', '„', 'a', 'tank', '##erületi', 'központ', 'sem', 'kíván', 'beleszól', '##ni', 'abba', ',', 'hogy', 'az', 'Önkormányzat', 'hogyan', 'és', 'kik', '##kel', 'látja', 'el', 'a', 'feladatait', '.', '”', 'A', 'tank', '##erületi', 'vezető', 'nem', 'látja', 'indokolt', '##nak', ',', 'hogy', 'személyesen', 'is', 'találkoz', '##zon', 'a', 'kerület', 'polgármester', '##ével', ',', 'mert', 'a', 'munkáltatói', 'intézkedés', '„', 'kizárólag', 'a', 'hatályos', 'jogszabályok', '##ból', 'fakadó', 'lépés', 'volt', '”', '.', 'Maros', '##i', 'szerint', 'az', 'érintett', 'pedagógusok', 'a', 'többszöri', 'figyelmeztetés', 'ellenére', 'is', 'visszatérő', '##en', 'megszeg', '##ték', '„', 'a', 'pedagógus', 'jogviszony', '##ból', 'fakadó', 'kötelezettségek', '##et', '”', '.', '„', 'Természetesen', 'köszönet', '##tel', 'veszem', 'Tisztelt', 'Polgármester', 'Úr', 'közreműködés', '##ét', ',', 'amennyiben', 'az', 'arra', 'irányul', ',', 'hogy', 'a', 'pedagógusok', 'a', 'jogszabályokban', 'számukra', 'biztosított', ',', 'törvényes', 'módon', 'feje', '##z', '##zék', 'ki', 'vélemény', '##üket', ',', 'úgy', ',', 'hogy', 'az', 'a', 'gyermekek', 'tanulás', '##hoz', 'való', 'jogának', 'érvényesül', '##ését', 'ne', 'korlátozza', '”', '–', 'fogalmaz', 'a', 'levélben', '.', 'Sopron', '##i', 'azért', 'kezdeményezett', 'beszélgetést', ',', 'mert', 'agg', '##asztja', 'a', 'kerületi', 'diákok', 'és', 'tanárok', 'sorsa', '.', '„', 'Mi', 'tesszük', 'tovább', 'a', 'dolgunk', '##at', ':', 'száz', '##millió', '##k', '##kal', 'támogat', '##juk', 'a', 'helyi', 'iskolák', 'felújítás', '##át', ',', 'cserébe', 'csak', 'a', 'tanárok', 'és', 'diákok', 'felé', 'mutatott', 'em', '##pat', '##ikus', 'hozzáállás', '##t', 'vár', '##nánk', '”', '–', 'írja', 'a', 'Facebook', '-', 'bejegyzés', '##ében', '.', 'Múlt', 'pénteken', 'a', 'VI', '.', 'kerületi', 'Kölcs', '##ey', 'Ferenc', 'Gimnázium', 'több', ',', 'a', 'polgári', 'enged', '##etlenség', '##ben', 'résztvevő', 'tanár', '##át', 'azonnali', 'hatállyal', 'men', '##esztett', '##ék', '.', 'A', 'tank', '##erület', 'azt', 'írta', 'a', 'Tel', '##ex', '##nek', ',', 'hogy', 'az', 'érintett', 'tanárok', 'felment', '##ése', 'indokolt', ',', 'mert', 'a', 'tanárok', 'írásbeli', 'figyelmeztetés', 'után', 'is', 'többször', 'megismétel', '##ték', '„', 'jogszerű', '##tlen', 'cselekedet', '##üket', '”', '.', 'Még', 'aznap', 'több', 'száz', '##an', 'tiltak', '##oztak', 'a', 'döntés', 'ellen', 'a', 'gimnázium', 'előtt', ',', 'majd', 'a', 'héten', 'tovább', 'folytató', '##d', '##tak', 'a', 'tüntet', '##ések', '.', 'A', 'men', '##esztett', 'tanárok', 'munkaügyi', 'pert', 'indít', '##anak', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "results = test_data.apply(generate_summary, axis=1)\n",
        "results = results.drop(['Textbody'], axis=1)\n",
        "results"
      ],
      "metadata": {
        "id": "rfSj4EbUQKoD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "outputId": "44da7a77-6085-4838-8798-e84199f940d3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py:1364: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                Lead  \\\n",
              "0  Letartóztatták Vietnám leggazdagabb és legtito...   \n",
              "1  Futóverseny miatt forgalomkorlátozások lesznek...   \n",
              "2  Egy futópár is feltartóztathatatlan, ha jól tá...   \n",
              "3  Orbán Viktor egykori iskolája is szolidaritást...   \n",
              "4  Perelnek a szülők, miután a magas energiaárakr...   \n",
              "5  Verstappen egy századdal nyerte az év legszoro...   \n",
              "6  Már tíz százalék feletti régióban vannak egyes...   \n",
              "7  A fő kérdés, hogy arányos válasz-e a tanárok k...   \n",
              "8  Márta István Vidnyánszkyról: Soha nem fogom me...   \n",
              "9  A Hatalom Gyűrűi kezd kifutni az időből, de ez...   \n",
              "\n",
              "                                        pred_summary  \n",
              "0  [A kínai ingatlanmágnás, Truong Kn - ven ( mag...  \n",
              "1  [A futóverseny miatt több forgalmas futóesemén...  \n",
              "2  [A sakkrejtvény - sorozat harmadik fordulójába...  \n",
              "3  [A székesfehérvári Teleki Blanka Gimnázium tan...  \n",
              "4  [A tankerület szerint ez nem a településre tar...  \n",
              "5  [A Mercedes kettős győzelmével zárult a 2022 -...  \n",
              "6  [A bankok fele már 10 százalék feletti kamatta...  \n",
              "7  [A Társaság a Szabadságjogokért ( TASZ ) jogvé...  \n",
              "8  [A fesztiválalapításról, a politikai hatalomró...  \n",
              "9  [A Hatalom Gyűrűi múlt heti eseményei után a v...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-93d7395f-dd4a-49af-a24b-c99cfb0034b0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Lead</th>\n",
              "      <th>pred_summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Letartóztatták Vietnám leggazdagabb és legtito...</td>\n",
              "      <td>[A kínai ingatlanmágnás, Truong Kn - ven ( mag...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Futóverseny miatt forgalomkorlátozások lesznek...</td>\n",
              "      <td>[A futóverseny miatt több forgalmas futóesemén...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Egy futópár is feltartóztathatatlan, ha jól tá...</td>\n",
              "      <td>[A sakkrejtvény - sorozat harmadik fordulójába...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Orbán Viktor egykori iskolája is szolidaritást...</td>\n",
              "      <td>[A székesfehérvári Teleki Blanka Gimnázium tan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Perelnek a szülők, miután a magas energiaárakr...</td>\n",
              "      <td>[A tankerület szerint ez nem a településre tar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Verstappen egy századdal nyerte az év legszoro...</td>\n",
              "      <td>[A Mercedes kettős győzelmével zárult a 2022 -...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Már tíz százalék feletti régióban vannak egyes...</td>\n",
              "      <td>[A bankok fele már 10 százalék feletti kamatta...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>A fő kérdés, hogy arányos válasz-e a tanárok k...</td>\n",
              "      <td>[A Társaság a Szabadságjogokért ( TASZ ) jogvé...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Márta István Vidnyánszkyról: Soha nem fogom me...</td>\n",
              "      <td>[A fesztiválalapításról, a politikai hatalomró...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>A Hatalom Gyűrűi kezd kifutni az időből, de ez...</td>\n",
              "      <td>[A Hatalom Gyűrűi múlt heti eseményei után a v...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-93d7395f-dd4a-49af-a24b-c99cfb0034b0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-93d7395f-dd4a-49af-a24b-c99cfb0034b0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-93d7395f-dd4a-49af-a24b-c99cfb0034b0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example of the lead, result and original text\n",
        "print('ORIGINAL LEAD')\n",
        "print(results.iloc[3]['Lead'])\n",
        "print(\"\")\n",
        "print('PREDICTION')\n",
        "print(results.iloc[3]['pred_summary'][0])\n",
        "print(\"\")\n",
        "print('OIRGINAL TEXT')\n",
        "test_data.iloc[3]['Textbody']\n"
      ],
      "metadata": {
        "id": "3sTJb5QO6k8r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "outputId": "90f0bddb-d25b-410e-e658-2f38491764f7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ORIGINAL LEAD\n",
            "Orbán Viktor egykori iskolája is szolidaritást vállal a kirúgott pedagógusokkal. A Teleki Blanka Gimnázium 55 tanára írta alá a közleményt.\n",
            "\n",
            "PREDICTION\n",
            "A székesfehérvári Teleki Blanka Gimnázium tanárai kiálltak a tanárok mellett.\n",
            "\n",
            "OIRGINAL TEXT\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A Székesfehérvári Teleki Blanka Gimnázium és Általános Iskola 55 tanára az intézmény hivatalos Facebook oldalán közzétett nyilatkozatukban álltak ki az elmúlt napokban kirúgott kollégáik mellett. Ebbe az iskolába járt Orbán Viktor, miniszterelnök is, aki tavaly májusban egy, az iskolánál készült videóban azt mondta, hogy hálával gondol a volt tanáraira, akik megpróbáltak belőle embert faragni.„Kiállunk a megfélemlítettek, a véleményüket vállalni nem merők, a meghurcoltak, az azonnali hatállyal munkából felmentettek mellett.”A közleményben, amit 48-an névvel, még heten név nélkül írtak alá, azt írják, hogy kiállnak mindenki mellett, akit elmúlt években, évtizedekben hátrány ért, mert a jobbá tétel szándékával fel mert szólalni az oktatásügy akkori vagy jelenlegi állapota ellen. Továbbá elutasítják azt, hogy az oktatás összetett és szerteágazó problémáit a megkérdezésünk nélkül, valódi párbeszéd nélkül, felülről, erőből, parancsszóval próbálják megoldani. Emellett felhívták arra a figyelmet, hogy szerintük az utolsó órában vagyunk, hozzáteszik, hogy ők maradni maradni szeretnének egy olyan iskolai rendszerben, ami tanárnak és diáknak is jó és hasznos. Ehhez kérték a társadalom támogatását. Kiemelték, hogy ez nem az iskola, hanem az aláírók álláspontja. Az elmúlt napokban egyre több helyről érkezik látványos kiállás a kirúgott, illetve megfélemlített pedagógusok mellett. A polgári engedetlenségben való részvétel miatt, a budapesti Kölcsey Ferenc Gimnáziumból kirúgott tanárok mellett tartottak tüntetést. Október 5-én, szerdán pedig országos sztrájkkal, több kilométeres élőlánccal, hídfoglalással és koncertekkel egybekötött tüntetéssel telt a nap, ekkor az egész ország a tanárokra figyelt. Péntek reggel a Külső-Pesti Tankerületi Központ Üllői úti épülete előtt tüntettek közel ezren, miközben a pestszentlőrinci Karinthy Frigyes Gimnázium 24 tanára átvette a figyelmeztető levelét a tankerülettől. Hétfőre pedig újabb tüntetés szerveződik, miután A Közép-Budai Tankerület pénteken figyelmeztető leveleket küldött sok, a polgári engedetlenségben résztvevő pedagógusnak – köztük olyannak is, akinek nem is volt órája az engedetlenség idején.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Rouge and human evaluation "
      ],
      "metadata": {
        "id": "DLhRYQj16JrF"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train further"
      ],
      "metadata": {
        "id": "-IQ1UmraYIWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#already default\n",
        "#model.config.decoder_start_token_id == tokenizer.cls_token_id\n",
        "#model.config.eos_token_id == tokenizer.sep_token_id\n",
        "#model.config.pad_token_id == tokenizer.pad_token_id\n",
        "#model.config.vocab_size == model.config.encoder.vocab_size"
      ],
      "metadata": {
        "id": "VCWk8auo-Z6w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1f00ab9-3375-47de-dd98-2e1e9366b734"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#még nem akarom állítgatni\n",
        "#model.config.max_length = 142\n",
        "#model.config.min_length = 56\n",
        "#model.config.no_repeat_ngram_size = 3\n",
        "#model.config.early_stopping = True\n",
        "#model.config.length_penalty = 2.0\n",
        "#model.config.num_beams = 4"
      ],
      "metadata": {
        "id": "ftHFWocn-_cB"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!rm seq2seq_trainer.py\n",
        "!rm seq2seq_training_args.py\n",
        "!wget https://raw.githubusercontent.com/huggingface/transformers/main/examples/legacy/seq2seq/seq2seq_trainer.py\n",
        "!wget https://raw.githubusercontent.com/huggingface/transformers/main/examples/legacy/seq2seq/seq2seq_training_args.py"
      ],
      "metadata": {
        "id": "1C0TSMQnFkX8"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install git-python==1.0.3\n",
        "!pip install rouge_score\n",
        "!pip install sacrebleu"
      ],
      "metadata": {
        "id": "Nn_rY9ayGo7_"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from seq2seq_trainer import Seq2SeqTrainer\n",
        "from seq2seq_training_args import Seq2SeqTrainingArguments"
      ],
      "metadata": {
        "id": "IQ5TlgFRG61z"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    predict_with_generate=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    fp16=True, \n",
        "    output_dir=\"./output\",\n",
        "    logging_steps=2,\n",
        "    save_steps=10,\n",
        "    eval_steps=4\n",
        "    # logging_steps=1000,\n",
        "    # save_steps=500,\n",
        "    # eval_steps=7500,\n",
        "    # warmup_steps=2000,\n",
        "    # save_total_limit=3,\n",
        ")"
      ],
      "metadata": {
        "id": "zBJzzbVVG9yZ"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "rouge = datasets.load_metric(\"rouge\")"
      ],
      "metadata": {
        "id": "YiMDKkfZMgGV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f85610ce-2218-479b-ce41-819313909228"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(pred):\n",
        "    labels_ids = pred.label_ids\n",
        "    pred_ids = pred.predictions\n",
        "\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
        "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
        "\n",
        "    rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
        "\n",
        "    return {\n",
        "        \"rouge2_precision\": round(rouge_output.precision, 4),\n",
        "        \"rouge2_recall\": round(rouge_output.recall, 4),\n",
        "        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n",
        "    }"
      ],
      "metadata": {
        "id": "BcmBQtA0LoHh"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=val_data,\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "Em_YUiEpMv0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        },
        "outputId": "449645ba-8e4b-4733-c3d0-60a74bcd5649"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cuda_amp half precision backend\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 20\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 10\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 10\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 6\n",
            "  Number of trainable parameters = 249636609\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-80f1b3ecf765>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0meval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m )\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1503\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1504\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1505\u001b[0;31m             \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1506\u001b[0m         )\n\u001b[1;32m   1507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1747\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m                 if (\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2507\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2508\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2510\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/seq2seq_trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/seq2seq_trainer.py\u001b[0m in \u001b[0;36m_compute_loss\u001b[0;34m(self, model, inputs, labels)\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0;31m# compute usual loss via models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;31m# compute label smoothed loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    626\u001b[0m             \u001b[0mpast_key_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs_decoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m         )\n\u001b[1;32m    630\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1239\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1241\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1242\u001b[0m         )\n\u001b[1;32m   1243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1022\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m         )\n\u001b[1;32m   1026\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    608\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m                 )\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    492\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself_attn_past_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m         )\n\u001b[1;32m    496\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m         )\n\u001b[1;32m    428\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    339\u001b[0m                 \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_scores\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrelative_position_scores_query\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrelative_position_scores_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_scores\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_head_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;31m# Apply the attention mask is (precomputed for all layers in BertModel forward() function)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 0; 14.76 GiB total capacity; 13.83 GiB already allocated; 29.75 MiB free; 13.93 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "utKitw18ja9z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}